{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23d8ac8c",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "In this notebook we demonstrate how to start the simulation from arbitrary SMPL(-X) pose using linear blend-skinning (LBS) initialization. If you don't want to use LBS (for example, you are using non-SMPL(-X) body meshes),  please see `Inference_from_mesh_sequence.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa3ba9-6b0f-4457-afb2-03016cadc4d5",
   "metadata": {},
   "source": [
    "## Choose a garment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22942f52-5854-4bdd-8d98-c0d06bf0a281",
   "metadata": {},
   "source": [
    "First, you need to choose a garment to simulate.\n",
    "\n",
    "Its template and some auxiliary data should be stored in the .pkl file under `DEFAULTS.data_root/aux_data/garment_dicts/`\n",
    "\n",
    "You can choose from the list of outfits already provided in this folder:\n",
    "\n",
    "![ccraft_garments](static/ccraft_garments.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69656be-c3d7-40e9-8605-fa79fb8ebdf1",
   "metadata": {},
   "source": [
    "Or you can import a new garment from an `.obj` file\n",
    "\n",
    "We also provide `.obj` files for all garments usen in the paper in `DEFAULTS.data_root/aux_data/garment_meshes/` directory.\n",
    "Note that these `.obj` files only have demonstrational purpose. \n",
    "For inference and training we use garment data stored in the .pkl files under `DEFAULTS.data_root/aux_data/garments_dicts/`, not the .obj files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c2e112-63eb-4484-a0b7-8678ac2a3630",
   "metadata": {},
   "source": [
    "## Or add your own garment from an `.obj` file\n",
    "\n",
    "[GarmentImport.ipynb](GarmentImport.ipynb) notebook demostrates how you can import garments from .obj files.\n",
    "\n",
    "To do this, you'll either need \n",
    "* a garment geometry aligned with the canonical SMPL(-X) body OR\n",
    "* a garment geometry aligned with an arbitrary SMPL(-X) body and the corresponging SMPL(-X) parameters.\n",
    "\n",
    "[GarmentImport.ipynb](GarmentImport.ipynb) discusses both of these cases.\n",
    "\n",
    "In the end you will get a .pkl file containing all information required to simulate your garment. We call such .pkl files \"garment dictionaries\" or \"garment dicts\".\n",
    "\n",
    "\n",
    "If you want to create an outfit from several garments whose geometries may intersect, use [Untanglement.ipynb](Untanglement.ipynb) to order and untangle their geometries and combine them into a single outfit. (You'' anyway first need to create garment dicts for each garment as described in GarmentImport.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab918a1-0c18-4e0c-a19f-dadd9eb9b44b",
   "metadata": {},
   "source": [
    "# Generate rollout for one sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef67ff-a467-4b3b-820c-83ab53342fb8",
   "metadata": {},
   "source": [
    "Once we have created a garment dict file for our garment (or you can use one of the garments that are already under `DEFAULTS.data_root/aux_data/garment_dicts/`), we can generate a rollout sequence for them using a trained HOOD/ContourCraft model.\n",
    "\n",
    "We provide 3 pretrained models and corresponding configuration files for each of them. The weights of the trained models are located in `DEFAULTS.data_root/trained_models`. The configuration files are in  `DEFAULTS.project_dir/configs`\n",
    "\n",
    "| model file      | config name           | comments                                                                                                                                                                                                                            |\n",
    "|-----------------|-----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| cvpr_submission | hood_cvpr                  | HOOD model used in the CVPR paper. No multi-layer simulation. Use it if you want to compare to the HOOD paper.                                                                                                                                                           |\n",
    "| postcvpr        | hood_final              | Newer HOOD model trained using refactored code with minor bug fixes. No multi-layer simulation. Use it if you want to use HOOD model in a downstream task.|\n",
    "| **contourcraft**        | **contourcraft**              | **Model from the ContourCraft paper. Can simulate multi-layer outfits**. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab84941",
   "metadata": {},
   "source": [
    "## Choose pose sequence "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a890b49",
   "metadata": {},
   "source": [
    "This repository supports inference over .npz sequences from CMU split of AMASS dataset.\n",
    "\n",
    "You can download them [here](https://amass.is.tue.mpg.de/). Use gendered SMPL+H sequences if you want to use SMPL model and gendered SMPL-X sequences if you want to use SMPL-X one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366caa43-f3e1-4ff2-9f4f-02087a09fc80",
   "metadata": {},
   "source": [
    "### create validation config and create `Runner` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3eb786-1682-4894-995a-366dd7c92e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/agrigorev/Workdir/contourcraft_private/utils/validation.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd = torch.load(checkpoint_path)\n"
     ]
    }
   ],
   "source": [
    "from utils.validation import apply_material_params\n",
    "from utils.validation import load_runner_from_checkpoint\n",
    "from utils.arguments import load_params\n",
    "from utils.common import move2device\n",
    "from utils.io import pickle_dump\n",
    "from utils.defaults import DEFAULTS\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Set material paramenters, see configs/cvpr.yaml for the training ranges for each parameter\n",
    "material_dict = dict()\n",
    "material_dict['density'] = 0.20022\n",
    "material_dict['lame_mu'] = 23600.0\n",
    "material_dict['lame_lambda'] = 44400\n",
    "material_dict['bending_coeff'] = 3.962e-05\n",
    "\n",
    "\n",
    "# ====================================================================================================\n",
    "\n",
    "models_dir = Path(DEFAULTS.data_root) / 'trained_models'\n",
    "\n",
    "# Choose the model and the configuration file\n",
    "\n",
    "# config_name = 'hood_cvpr'\n",
    "# checkpoint_path = models_dir / 'hood_cvpr.pth'\n",
    "\n",
    "# config_name = 'hood_final'\n",
    "# checkpoint_path = models_dir / 'hood_final.pth'\n",
    "\n",
    "config_name = 'contourcraft'\n",
    "checkpoint_path = models_dir / 'contourcraft.pth'\n",
    "\n",
    "\n",
    "# ====================================================================================================\n",
    "\n",
    "\n",
    "# load the config from .yaml file and load .py modules specified there\n",
    "modules, experiment_config = load_params(config_name)\n",
    "\n",
    "# modify the config to use it in validation \n",
    "experiment_config = apply_material_params(experiment_config, material_dict)\n",
    "\n",
    "# load Runner object and the .py module it is declared in\n",
    "runner_module, runner = load_runner_from_checkpoint(checkpoint_path, modules, experiment_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d6421-9e94-41a9-83fb-6373ef431e18",
   "metadata": {},
   "source": [
    "### create one-sequence dataloader\n",
    "\n",
    "Here you'll need to choose a garment by setting `garment_name` variable. The garment name should correspond to a `.pkl` file under `DEFAULTS.data_root/aux_data/garments_dicts/`\n",
    "\n",
    "Note that it can also be a comma-separated list of garments, then they'll be combined into a single outfit. For example:\n",
    "```\n",
    "garment_name = 'smplx/cindy_020::bottom_skirt, smplx/cindy_020::top_blouse'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd0e9c8",
   "metadata": {},
   "source": [
    "**To test for SMPL poses, use**\n",
    "\n",
    "```\n",
    "sequence_path =  'path/to/SMPLH/AMASS/CMU/01/01_01_poses.npz'\n",
    "garment_name = 'longskirt'\n",
    "sequence_loader = 'cmu_npz_smpl'\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e00cfc-0602-4109-9817-5d0690ab0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file with the pose sequence\n",
    "from utils.validation import create_postcvpr_one_sequence_dataloader\n",
    "\n",
    "# If True, the SMPL(-X) poses are slightly modified to avoid hand-body self-penetrations. The technique is adopted from the code of SNUG \n",
    "separate_arms = True\n",
    "\n",
    "# path to the pose sequence file\n",
    "CMU_path = 'path/to/AMASS/CMU'\n",
    "sequence_path =  Path(CMU_path) / '01/01_01_stageii.npz'\n",
    "\n",
    "# name of the garment to simulate \n",
    "# should correspond to a pkl file under DEFAULTS.data_root/aux_data/garments_dicts/\n",
    "garment_name = 'smplx/cindy_020_combined_test'\n",
    "\n",
    "# It can be a comma-separated list of individual garments\n",
    "# garment_name = 'smplx/cindy_020::bottom_skirt, smplx/cindy_020::top_blouse'\n",
    "\n",
    "\n",
    "# gender of the body model, sould be the same as the one used to create the garment\n",
    "gender = 'female'\n",
    "\n",
    "# ====================================================================================================\n",
    "\n",
    "# Choose the type of the pose sequence you want to use: 'cmu_npz_smpl', 'cmu_npz_smplx'\n",
    "\n",
    "# to use AMASS SMPL-X pose sequence\n",
    "sequence_loader = 'cmu_npz_smplx'\n",
    "\n",
    "# to use AMASS SMPL pose sequence\n",
    "# sequence_loader = 'cmu_npz_smpl\n",
    "\n",
    "\n",
    "\n",
    "# ====================================================================================================\n",
    "\n",
    "\n",
    "dataloader = create_postcvpr_one_sequence_dataloader(sequence_path, garment_name, sequence_loader=sequence_loader, \n",
    "                                            obstacle_dict_file=None, gender=gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29d952c3-0def-410e-b152-e9e3dd99e11a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:38<00:00,  5.22it/s]\n"
     ]
    }
   ],
   "source": [
    "sequence = next(iter(dataloader))\n",
    "sequence = move2device(sequence, 'cuda:0')\n",
    "trajectories_dict = runner.valid_rollout(sequence,  bare=True, n_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3476f3-3b26-4224-b27a-4cbfc4ed0001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rollout saved into /data/agrigorev/02_Projects/ccraft_data/temp/output_hood_old.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the sequence to disk\n",
    "out_path = Path(DEFAULTS.data_root) / 'temp' / 'output.pkl'\n",
    "print(f\"Rollout saved into {out_path}\")\n",
    "pickle_dump(dict(trajectories_dict), out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733d150e-aa88-420a-b22c-02c561f205aa",
   "metadata": {},
   "source": [
    "### write a video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64485f0f-55ef-45e2-b64a-9ef2e5fc27c6",
   "metadata": {},
   "source": [
    "Finally, we can render a video of the generated sequence with [aitviewer](https://github.com/eth-ait/aitviewer)\n",
    "\n",
    "Or you can render it interactively using `python utils/show.py rollout_path=PATH_TO_SEQUENCE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e8d616f-f90f-4c19-bc9a-6980da5718e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/agrigorev/miniforge3/envs/ccraft/lib/python3.10/site-packages/trimesh/geometry.py:7: UserWarning: A NumPy version >=1.23.5 and <2.5.0 is required for this version of SciPy (detected version 1.23.1)\n",
      "  import scipy.sparse\n"
     ]
    }
   ],
   "source": [
    "from utils.show import write_video \n",
    "from aitviewer.headless import HeadlessRenderer\n",
    "\n",
    "# Careful!: creating more that one renderer in a single session causes an error\n",
    "renderer = HeadlessRenderer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c43b212-2ec8-4459-9442-a1dae7eb349c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out_path \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(DEFAULTS\u001b[38;5;241m.\u001b[39mdata_root) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m out_video \u001b[38;5;241m=\u001b[39m Path(DEFAULTS\u001b[38;5;241m.\u001b[39mdata_root) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m write_video(out_path, out_video, renderer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "out_path = Path(DEFAULTS.data_root) / 'temp' / 'output.pkl'\n",
    "out_video = Path(DEFAULTS.data_root) / 'temp' / 'output.mp4'\n",
    "write_video(out_path, out_video, renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9658210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccraft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
