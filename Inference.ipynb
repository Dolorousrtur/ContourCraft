{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23d8ac8c",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "In this notebook we demonstrate how to start the simulation from an arbitrary SMPL(-X) pose using linear blend-skinning (LBS) initialization. If you don't want to use LBS (for example, you are using non-SMPL(-X) body meshes),  please see `Inference_from_mesh_sequence.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa3ba9-6b0f-4457-afb2-03016cadc4d5",
   "metadata": {},
   "source": [
    "## Choose a garment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22942f52-5854-4bdd-8d98-c0d06bf0a281",
   "metadata": {},
   "source": [
    "First, you need to choose a garment to simulate.\n",
    "\n",
    "Its template and some auxiliary data should be stored in the .pkl file under `DEFAULTS.data_root/aux_data/garment_dicts/`\n",
    "\n",
    "You can choose from the list of outfits already provided in this folder:\n",
    "\n",
    "![ccraft_garments](static/ccraft_garments.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69656be-c3d7-40e9-8605-fa79fb8ebdf1",
   "metadata": {},
   "source": [
    "Or you can import a new garment from an `.obj` file\n",
    "\n",
    "We also provide `.obj` files for all garments used in the paper in the `DEFAULTS.data_root/aux_data/garment_meshes/` directory.\n",
    "Note that these `.obj` files only have demonstrational purpose. \n",
    "For inference and training we use garment data stored in the .pkl files under `DEFAULTS.data_root/aux_data/garment_dicts/`, not the .obj files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c2e112-63eb-4484-a0b7-8678ac2a3630",
   "metadata": {},
   "source": [
    "## Or add your own garment from an `.obj` file\n",
    "\n",
    "[GarmentImport.ipynb](GarmentImport.ipynb) notebook demostrates how you can import garments from .obj files.\n",
    "\n",
    "To do this, you'll either need \n",
    "* a garment geometry aligned with the canonical SMPL(-X) body OR\n",
    "* a garment geometry aligned with an arbitrary SMPL(-X) body and the corresponging SMPL(-X) parameters.\n",
    "\n",
    "[GarmentImport.ipynb](GarmentImport.ipynb) discusses both of these cases.\n",
    "\n",
    "In the end you will get a .pkl file containing all information required to simulate your garment. We call such .pkl files \"garment dictionaries\" or \"garment dicts\".\n",
    "\n",
    "\n",
    "If you want to create an outfit from several garments whose geometries may intersect, use [Untanglement.ipynb](Untanglement.ipynb) to order and untangle their geometries and combine them into a single outfit. (You'll anyway first need to create garment dicts for each garment as described in GarmentImport.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab918a1-0c18-4e0c-a19f-dadd9eb9b44b",
   "metadata": {},
   "source": [
    "# Generate a trajectory for a single sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef67ff-a467-4b3b-820c-83ab53342fb8",
   "metadata": {},
   "source": [
    "Once we have created a garment dict file for our garment (or you can use one of the garments that are already under `DEFAULTS.data_root/aux_data/garment_dicts/`), we can generate a trajectory using a trained HOOD/ContourCraft model.\n",
    "\n",
    "We provide 3 pretrained models and corresponding configuration files for each of them. The weights of the trained models are located in `DEFAULTS.data_root/trained_models`. The configuration files are in  `DEFAULTS.project_dir/configs`\n",
    "\n",
    "| model file      | config name           | comments                                                                                                                                                                                                                            |\n",
    "|-----------------|-----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| hood_cvpr | hood_cvpr                  | The HOOD model used in the CVPR paper. No multi-layer simulation. Use it if you want to compare to the HOOD paper.                                                                                                                                                           |\n",
    "| hood_final        | hood_final              | A newer HOOD model trained using refactored code with minor bug fixes. No multi-layer simulation. Use it if you want to use HOOD model in a downstream task.|\n",
    "| **contourcraft**        | **contourcraft**              | **Model from the ContourCraft paper. Can simulate multi-layer outfits**. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab84941",
   "metadata": {},
   "source": [
    "## Choose pose sequence "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a890b49",
   "metadata": {},
   "source": [
    "This repository supports inference over .npz sequences from the CMU split of AMASS dataset.\n",
    "\n",
    "You can download them [here](https://amass.is.tue.mpg.de/). Use gendered SMPL+H sequences if you want to use the SMPL model or gendered SMPL-X sequences if you want to use the SMPL-X one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366caa43-f3e1-4ff2-9f4f-02087a09fc80",
   "metadata": {},
   "source": [
    "### create a validation config and a `Runner` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3eb786-1682-4894-995a-366dd7c92e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.validation import apply_material_params\n",
    "from utils.validation import load_runner_from_checkpoint\n",
    "from utils.arguments import load_params\n",
    "from utils.common import move2device\n",
    "from utils.io import pickle_dump\n",
    "from utils.defaults import DEFAULTS\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Set material paramenters, see configs/contourcraft.yaml for the training ranges for each parameter\n",
    "material_dict = dict()\n",
    "material_dict['density'] = 0.20022\n",
    "material_dict['lame_mu'] = 23600.0\n",
    "material_dict['lame_lambda'] = 44400\n",
    "material_dict['bending_coeff'] = 3.962e-05\n",
    "\n",
    "\n",
    "# ====================================================================================================\n",
    "\n",
    "models_dir = Path(DEFAULTS.data_root) / 'trained_models'\n",
    "\n",
    "# Choose the model and the configuration file\n",
    "\n",
    "# config_name = 'hood_cvpr'\n",
    "# checkpoint_path = models_dir / 'hood_cvpr.pth'\n",
    "\n",
    "# config_name = 'hood_final'\n",
    "# checkpoint_path = models_dir / 'hood_final.pth'\n",
    "\n",
    "config_name = 'contourcraft'\n",
    "checkpoint_path = models_dir / 'contourcraft.pth'\n",
    "\n",
    "\n",
    "# ====================================================================================================\n",
    "\n",
    "\n",
    "# load the config from a .yaml file and load .py modules specified there\n",
    "modules, experiment_config = load_params(config_name)\n",
    "\n",
    "# modify the config to use it for validation \n",
    "experiment_config = apply_material_params(experiment_config, material_dict)\n",
    "\n",
    "# load a Runner object and the .py module it is declared in\n",
    "runner_module, runner = load_runner_from_checkpoint(checkpoint_path, modules, experiment_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d6421-9e94-41a9-83fb-6373ef431e18",
   "metadata": {},
   "source": [
    "### create one-sequence dataloader\n",
    "\n",
    "Here you'll need to choose a garment by setting the `garment_name` variable. The garment name should correspond to a `.pkl` file under `garment_dicts_dir` (defined below)\n",
    "\n",
    "Note that it can also be a comma-separated list of garments. In this case, they'll be combined into a single outfit. For example:\n",
    "```\n",
    "garment_name = 'cindy_020::bottom_skirt, cindy_020::top_blouse'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e00cfc-0602-4109-9817-5d0690ab0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file with the pose sequence\n",
    "from utils.validation import create_postcvpr_one_sequence_dataloader\n",
    "\n",
    "# If True, the SMPL(-X) poses are slightly modified to avoid hand-body self-penetrations. The technique is adopted from the code of SNUG \n",
    "separate_arms = True\n",
    "\n",
    "# To test the simulation with the SMPL body model\n",
    "# CMU_path = 'path/to/AMASS/smpl/CMU'\n",
    "# sequence_path =  Path(CMU_path) / '01/01_01_poses.npz'\n",
    "# sequence_loader = 'cmu_npz_smpl'\n",
    "# garment_dicts_dir = Path(DEFAULTS.aux_data) / 'garment_dicts' / 'smpl' \n",
    "# garment_name = 'hooded_tight_dress'\n",
    "# gender = 'female'\n",
    "\n",
    "# To test the simulation with the SMPL-X body model\n",
    "CMU_path = 'path/to/AMASS/smplx/CMU'\n",
    "sequence_path =  Path(CMU_path) / '01/01_01_stageii.npz'\n",
    "sequence_loader = 'cmu_npz_smplx'\n",
    "garment_dicts_dir = Path(DEFAULTS.aux_data) / 'garment_dicts' / 'smplx' \n",
    "garment_name = \"celina_002_combined\"\n",
    "gender = 'female'\n",
    "\n",
    "dataloader = create_postcvpr_one_sequence_dataloader(sequence_path, garment_name, sequence_loader=sequence_loader, \n",
    "                                            obstacle_dict_file=None, gender=gender, garment_dicts_dir=garment_dicts_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d952c3-0def-410e-b152-e9e3dd99e11a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequence = next(iter(dataloader))\n",
    "sequence = move2device(sequence, 'cuda:0')\n",
    "trajectories_dict = runner.valid_rollout(sequence,  bare=True, n_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3476f3-3b26-4224-b27a-4cbfc4ed0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sequence to disk\n",
    "out_path = Path(DEFAULTS.data_root) / 'temp' / 'output.pkl'\n",
    "print(f\"Rollout saved into {out_path}\")\n",
    "pickle_dump(dict(trajectories_dict), out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733d150e-aa88-420a-b22c-02c561f205aa",
   "metadata": {},
   "source": [
    "### write a video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64485f0f-55ef-45e2-b64a-9ef2e5fc27c6",
   "metadata": {},
   "source": [
    "Finally, we can render a video of the generated sequence with [aitviewer](https://github.com/eth-ait/aitviewer)\n",
    "\n",
    "Or you can render it interactively using `python utils/show.py rollout_path=PATH_TO_SEQUENCE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d616f-f90f-4c19-bc9a-6980da5718e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.show import write_video \n",
    "from aitviewer.headless import HeadlessRenderer\n",
    "\n",
    "# Careful!: creating more that one renderer in a single session causes an error\n",
    "renderer = HeadlessRenderer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43b212-2ec8-4459-9442-a1dae7eb349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = Path(DEFAULTS.data_root) / 'temp' / 'output.pkl'\n",
    "out_video = Path(DEFAULTS.data_root) / 'temp' / 'output.mp4'\n",
    "write_video(out_path, out_video, renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9658210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccraft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
