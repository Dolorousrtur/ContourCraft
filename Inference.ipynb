{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "413e290d-dedd-48c3-893b-c5641ce9f2b7",
   "metadata": {},
   "source": [
    "# set enviromental variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1ee6e6-0226-4b7b-bbe7-3a4b39a7e85c",
   "metadata": {},
   "source": [
    "Make sure you set these two enviromental variables:\n",
    "\n",
    "* `HOOD_PROJECT` should lead to the HOOD repository\n",
    "* `HOOD_DATA` should lead to the data folder (see `README.md` for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590cf469-7988-4bd3-a95b-00290785e5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HOOD_PROJECT =  \"/path/to/hood/repository\"\n",
    "HOOD_DATA = \"/path/to/hood/data\"\n",
    "\n",
    "os.environ[\"HOOD_PROJECT\"] = HOOD_PROJECT\n",
    "os.environ[\"HOOD_DATA\"] = HOOD_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa3ba9-6b0f-4457-afb2-03016cadc4d5",
   "metadata": {},
   "source": [
    "# Choose a garment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22942f52-5854-4bdd-8d98-c0d06bf0a281",
   "metadata": {},
   "source": [
    "Next, you need to choose a garment to simulate.\n",
    "\n",
    "Its template and some auxiliary data should be stored in the `$HOOD_DATA/aux_data/garments_dict.pkl` file\n",
    "\n",
    "You can choose from the list of outfits already provided in this file:\n",
    "\n",
    "![ccraft_garments](static/ccraft_garments.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69656be-c3d7-40e9-8605-fa79fb8ebdf1",
   "metadata": {},
   "source": [
    "Or you can import a new garment from an `.obj` file\n",
    "\n",
    "We also provide `.obj` files for all garments usen in the paper in `$HOOD_DATA/aux_data/garment_meshes/` directory.\n",
    "Note that these `.obj` files only have demonstrational purpose. \n",
    "For inference and training we use garment data stored in  `$HOOD_DATA/aux_data/garments_dict.pkl`, not the .obj files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c2e112-63eb-4484-a0b7-8678ac2a3630",
   "metadata": {},
   "source": [
    "## Add your own garment from an `.obj` file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb91aa",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate how to start the simulation from arbitrary SMPL(-X) pose using linear blend-skinning (LBS) initialization. If you don't want to use LBS (for example, you are using non-SMPL(-X) body meshes),  plase see `Inference_from_any_pose.ipynb.\n",
    "\n",
    "To add a new garment, you should have an `.obj` file with the garment mesh that is **aligned with the canonical body model** (zero-pose and zero-shape) you want to use.\n",
    "\n",
    "We currently support [SMPL](https://smpl.is.tue.mpg.de/) and [SMPL-X](https://smpl-x.is.tue.mpg.de/) body models.\n",
    "\n",
    "Note: you will not be able to run LBS-initialized simulation with a body model that is different from the one you specify in this step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f6ed06-b020-48fe-8e32-034957110def",
   "metadata": {},
   "source": [
    "First, add the garment to the `garments_dict.pkl` file using `add_garment_to_garments_dict` function.\n",
    "\n",
    "It builds a dictionary for the garment that contains:\n",
    "* `rest_pos`: \\[Nx3\\], positions of the vertices in canonical pose that are aligned to zero- pose and shape SMPL body.\n",
    "* `faces`: \\[Fx3\\], triplets of node indices that constitute each face\n",
    "* `node_type` \\[Nx1\\], node type labels (`0` for regular, `3` for \"pinned\"). By default, all nodes are regular, we show how to add \"pinned nodes\" later in this notebook\n",
    "* `lbs` dictionary with shape- and pose- blendshapes and skinning weights for the garment, sampled from SMPL model\n",
    "* `center` and `coarse_edges` info on long-range (coarse) edges used to build hiererchical graph of the garment.\n",
    "\n",
    "To be able to start simulation from arbitrary pose, we use linear blend-skinning (LBS) to initialize the garment geometry in the first frame. For each garment node we sample pose- and shape-blendshapes and skinning weights from the closest SMPL(-X) node in canonical pose.\n",
    "\n",
    "However, for loose garments such approach may result in overly-stretched triangles. Therefore, we use the approach introduced in [\\[Santesteban et al. 2021\\]](http://mslab.es/projects/SelfSupervisedGarmentCollisions/) and average skinning weights and blendshapes over many randomly sampled SMPL(-X) nodes around the given garment node.\n",
    "\n",
    "The parameter `n_samples_lbs` controls the number of random samples to use. We recommend setting it to 0 for tight-fitting garments (shirts, pants) and to 1000 for loose ones (skirts, dresses).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f305896c",
   "metadata": {},
   "source": [
    "**To test for SMPL poses, use**\n",
    "\n",
    "```\n",
    "garment_obj_path = os.path.join(DEFAULTS.aux_data, 'garment_meshes', 'hood', 'longskirt.obj')\n",
    "model_type = 'smpl'\n",
    "garments_dict_path = os.path.join(DEFAULTS.aux_data, 'garments_dict.pkl')\n",
    "garment_name = 'longskirt'\n",
    "\n",
    "pinned_indices = \\\n",
    "[2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215,\n",
    " 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235,\n",
    " 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2327, 2328, 2351, 2492, 2497, 2669, 2670, 2671, 2674, 2837, 3139,\n",
    " 3200, 3204, 3359, 3362, 3495, 3512, 3634, 3638, 3805, 3965, 3967, 4133, 4137, 4140, 4335, 4340, 4506, 4509, 4669, 4674,\n",
    " 4749, 4812, 4849, 4853, 5138, 5309, 5342, 5469, 5474, 5503, 5646, 5650, 5654, 5855, 5857, 6028, 6091, 6204, 6209, 6280,\n",
    " 6374, 6377, 6378, 6473, 6649, 6654, 6814, 6817, 6986, 6989, 6990, 6992, 7172, 7178, 7336, 7500, 7505, 7661, 7665, 7666]\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468c0e51-2d23-46e2-a789-e7757c113292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.mesh_creation import GarmentCreator, add_pinned_verts\n",
    "from utils.defaults import DEFAULTS\n",
    "\n",
    "\n",
    "garment_obj_path = os.path.join(DEFAULTS.aux_data, 'garment_meshes', 'ccraft', 'cindy_020_combined.obj')\n",
    "\n",
    "body_models_root = os.path.join(DEFAULTS.aux_data, 'body_models')\n",
    "\n",
    "# Model type, either 'smpl' or 'smplx'\n",
    "model_type = 'smplx'\n",
    "\n",
    "# gender, either 'male`, `female` or `neutral`\n",
    "gender = 'female'\n",
    "\n",
    "# garments_dict_path = os.path.join(DEFAULTS.aux_data, 'garments_dict_smplx.pkl')\n",
    "garments_dict_path = os.path.join(DEFAULTS.aux_data, 'garments_dict.pkl')\n",
    "\n",
    "# Name of the garment to add\n",
    "garment_name = 'cindy_020_combined_test'\n",
    "\n",
    "# Use approximate_center=True to create temlate faster. In the original paper code it was False\n",
    "gc = GarmentCreator(garments_dict_path, body_models_root, model_type, gender, \n",
    "                    n_samples_lbs=0, verbose=True, coarse=True, approximate_center=True)\n",
    "gc.add_garment(garment_obj_path, garment_name)\n",
    "\n",
    "# Now garment 'cindy_020_combined_test' is added to the garments_dict.pkl file and can be used in furter steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff85fb86-598b-4b48-baa8-903a89b84636",
   "metadata": {},
   "source": [
    "### Add pinned vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e9df6b-44c8-4c7c-a3d3-3b60d0e1e4bf",
   "metadata": {},
   "source": [
    "For some gaments, it is necessary to fix positions for a subset of nodes relative to the body. For example, fix the top ring of a skirt or pants to prevent it from falling off the body.\n",
    "\n",
    "To label a set of garment nodes as \"pinned\", you need to use `add_pinned_verts` function and provide it with the list of node indices that you want to pin.\n",
    "\n",
    "One easy way of getting indices for a set of nodes, is by using [Blender](https://www.blender.org/). \n",
    "\n",
    "1. Open it, import the garment from the `.obj` file. \n",
    "2. Then in `Layout` tab press `Tab` to go into the editing mode. \n",
    "3. Select all vertices you want to pin. \n",
    "4. Then, go into `Scripting` tab and execute the following piece of code there.\n",
    "\n",
    "```python\n",
    "import bpy\n",
    "import bmesh\n",
    "\n",
    "obj = bpy.context.active_object\n",
    "bm = bmesh.from_edit_mesh(obj.data)    \n",
    "obj = bpy.context.active_object; bm = bmesh.from_edit_mesh(obj.data)    ; selected = [i.index for i in bm.verts if i.select == True]; print(selected)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "5. You will get a list of indices for the selected nodes.\n",
    "\n",
    "Below are pinned indices for the garment `cindy_020_combined`, replace them with yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f4d502-920d-44b2-bcf1-ca0814057c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinned_indices = \\\n",
    "[   26,    27,    28,    36,    47,    56,    68,    69,    79,\n",
    "          93,    94,   103,   121,   130,   151,   161,   185,   197,\n",
    "         222,   237,   262,   279,   280,   307,   326,   353,   372,\n",
    "         402,   422,   455,   456,   478,   513,   538,   576,   603,\n",
    "         641,   670,   706,   738,   778,   812,   855,   891,   936,\n",
    "         973,  1020,  1061,  1109,  1151,  1200,  1243,  1293,  1339,\n",
    "        1390,  1439,  1492,  1543,  1597,  1651,  1706,  1761,  1821,\n",
    "        1878,  1936,  1994,  2052,  2112,  2176,  2238,  2306,  2369,\n",
    "        2439,  2508,  2576,  2644,  2713,  2719,  2784,  2858,  2926,\n",
    "        3001,  3071,  3148,  3218,  3296,  3368,  3448,  3522,  3607,\n",
    "        3687,  3770,  3850,  3934,  4016,  4102,  4187,  4269,  6567,\n",
    "        6626,  7071,  7368,  7577,  7804,  7920,  8040,  8460,  8550,\n",
    "        8632,  8992,  9065,  9137, 11694, 11695, 11700, 11706, 11712,\n",
    "       11715, 11726, 11731, 11735, 11739, 11743, 11747, 11755, 11761,\n",
    "       11770, 11779, 11792, 11797, 11802, 11814, 11819, 11824, 11829,\n",
    "       11851, 11857]\n",
    "\n",
    "add_pinned_verts(garments_dict_path, garment_name, pinned_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab918a1-0c18-4e0c-a19f-dadd9eb9b44b",
   "metadata": {},
   "source": [
    "# Generate rollout for one sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef67ff-a467-4b3b-820c-83ab53342fb8",
   "metadata": {},
   "source": [
    "Once we added our garment to the `garments_dict.pkl` (or you can use one of the garments that are already there), we can generate a rollout sequence for them using a trained HOOD/ContourCraft model.\n",
    "\n",
    "We provide 4 pretrained models and corresponding configuration files for each of them. The weights of the trained models are located in `$HOOD_DATA/trained_models`. The configuration files are in  `$HOOD_PROJECT/configs`\n",
    "\n",
    "| model file      | config name           | comments                                                                                                                                                                                                                            |\n",
    "|-----------------|-----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| cvpr_submission | hood_cvpr                  | HOOD model used in the CVPR paper. No multi-layer simulation. Use it if you want to compare to the HOOD paper.                                                                                                                                                           |\n",
    "| postcvpr        | hood_final              | Newer HOOD model trained using refactored code with minor bug fixes. No multi-layer simulation. Use it if you want to use HOOD model in a downstream task.|\n",
    "| **contourcraft**        | **contourcraft**              | **Model from the ContourCraft paper. Can simulate multi-layer outfits**. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab84941",
   "metadata": {},
   "source": [
    "## Choose pose sequence "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a890b49",
   "metadata": {},
   "source": [
    "This repository supports inference over .npz sequences from CMU split of AMASS dataset.\n",
    "\n",
    "You can download them [here](https://amass.is.tue.mpg.de/). Use SMPL+H sequences if you want to use SMPL model and SMPL-X sequences if you want to use SMPL-X one.\n",
    "\n",
    "Previously, we relied on our own .pkl format for pose sequences which is still used in some parts of the repository (e.g. to create validation sequences). To create these .pkl sequences, please, see `additional_notebooks/PoseSequences.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366caa43-f3e1-4ff2-9f4f-02087a09fc80",
   "metadata": {},
   "source": [
    "### create validation config and create `Runner` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3eb786-1682-4894-995a-366dd7c92e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.validation import apply_material_params\n",
    "from utils.validation import load_runner_from_checkpoint\n",
    "from utils.arguments import load_params\n",
    "from utils.common import move2device\n",
    "from utils.io import pickle_dump\n",
    "from utils.defaults import DEFAULTS\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Set material paramenters, see configs/cvpr.yaml for the training ranges for each parameter\n",
    "material_dict = dict()\n",
    "material_dict['density'] = 0.20022\n",
    "material_dict['lame_mu'] = 23600.0\n",
    "material_dict['lame_lambda'] = 44400\n",
    "material_dict['bending_coeff'] = 3.962e-05\n",
    "\n",
    "\n",
    "# ====================================================================================================\n",
    "\n",
    "models_dir = Path(DEFAULTS.data_root) / 'trained_models'\n",
    "\n",
    "# Choose the model and the configuration file\n",
    "\n",
    "# config_name = 'hood_cvpr'\n",
    "# checkpoint_path = models_dir / 'hood_cvpr.pth'\n",
    "\n",
    "# config_name = 'hood_final'\n",
    "# checkpoint_path = models_dir / 'hood_final.pth'\n",
    "\n",
    "config_name = 'contourcraft'\n",
    "checkpoint_path = models_dir / 'contourcraft.pth'\n",
    "\n",
    "\n",
    "# ====================================================================================================\n",
    "\n",
    "\n",
    "# load the config from .yaml file and load .py modules specified there\n",
    "modules, experiment_config = load_params(config_name)\n",
    "\n",
    "# modify the config to use it in validation \n",
    "experiment_config = apply_material_params(experiment_config, material_dict)\n",
    "\n",
    "# load Runner object and the .py module it is declared in\n",
    "runner_module, runner = load_runner_from_checkpoint(checkpoint_path, modules, experiment_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d6421-9e94-41a9-83fb-6373ef431e18",
   "metadata": {},
   "source": [
    "### create one-sequence dataloader\n",
    "\n",
    "Here you'll need to choose a garment from `garments_dict.pkl` by setting `garment_name` variable.\n",
    "\n",
    "Note that it can also be a comma-separated list of garments, then they'll be combined into a single outfit. For example:\n",
    "```\n",
    "garment_name = 'cindy_020::bottom_skirt, cindy_020::top_blouse'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd0e9c8",
   "metadata": {},
   "source": [
    "**To test for SMPL poses, use**\n",
    "\n",
    "```\n",
    "sequence_path =  'path/to/SMPLH/AMASS/CMU/01/01_01_poses.npz'\n",
    "garment_name = 'longskirt'\n",
    "sequence_loader = 'cmu_npz_smpl'\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e00cfc-0602-4109-9817-5d0690ab0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file with the pose sequence\n",
    "from utils.validation import create_postcvpr_one_sequence_dataloader\n",
    "\n",
    "# If True, the SMPL(-X) poses are slightly modified to avoid hand-body self-penetrations. The technique is adopted from the code of SNUG \n",
    "separate_arms = True\n",
    "\n",
    "# path to the pose sequence file\n",
    "sequence_path =  'path/to/AMASS/CMU/01/01_01_stageii.npz'\n",
    "\n",
    "# name of the garment to simulate\n",
    "garment_name = 'cindy_020_combined_test'\n",
    "\n",
    "# It can be a comma-separated list of individual garments\n",
    "# garment_name = 'cindy_020::bottom_skirt, cindy_020::top_blouse'\n",
    "\n",
    "garment_dict_file = 'garments_dict.pkl'\n",
    "\n",
    "\n",
    "# gender of the body model, sould be the same as the one used to create the garment\n",
    "gender = 'female'\n",
    "\n",
    "# ====================================================================================================\n",
    "\n",
    "# Choose the type of the pose sequence you want to use: 'cmu_npz_smpl', 'cmu_npz_smplx', 'hood_pkl'\n",
    "\n",
    "# to use AMASS SMPL-X pose sequence\n",
    "sequence_loader = 'cmu_npz_smplx'\n",
    "\n",
    "# to use AMASS SMPL pose sequence\n",
    "# sequence_loader = 'cmu_npz_smpl\n",
    "\n",
    "# to use our (legacy) SMPL pose sequence\n",
    "# sequence_loader = 'hood_pkl'\n",
    "\n",
    "\n",
    "# ====================================================================================================\n",
    "\n",
    "\n",
    "dataloader = create_postcvpr_one_sequence_dataloader(sequence_path, garment_name, \n",
    "                                            garment_dict_file, sequence_loader=sequence_loader, \n",
    "                                            obstacle_dict_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d952c3-0def-410e-b152-e9e3dd99e11a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequence = next(iter(dataloader))\n",
    "sequence = move2device(sequence, 'cuda:0')\n",
    "trajectories_dict = runner.valid_rollout(sequence,  bare=True, n_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3476f3-3b26-4224-b27a-4cbfc4ed0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sequence to disk\n",
    "out_path = Path(DEFAULTS.data_root) / 'temp' / 'output.pkl'\n",
    "print(f\"Rollout saved into {out_path}\")\n",
    "pickle_dump(dict(trajectories_dict), out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733d150e-aa88-420a-b22c-02c561f205aa",
   "metadata": {},
   "source": [
    "### write video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64485f0f-55ef-45e2-b64a-9ef2e5fc27c6",
   "metadata": {},
   "source": [
    "Finally, we can render a video of the generated sequence with [aitviewer](https://github.com/eth-ait/aitviewer)\n",
    "\n",
    "Or you can render it interactively using `python utils/show.py rollout_path=PATH_TO_SEQUENCE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d616f-f90f-4c19-bc9a-6980da5718e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.show import write_video \n",
    "from aitviewer.headless import HeadlessRenderer\n",
    "\n",
    "# Careful!: creating more that one renderer in a single session causes an error\n",
    "renderer = HeadlessRenderer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43b212-2ec8-4459-9442-a1dae7eb349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = Path(DEFAULTS.data_root) / 'temp' / 'output.pkl'\n",
    "out_video = Path(DEFAULTS.data_root) / 'temp' / 'output.mp4'\n",
    "write_video(out_path, out_video, renderer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
