{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a288d8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa3ba9-6b0f-4457-afb2-03016cadc4d5",
   "metadata": {},
   "source": [
    "# Choose a garment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22942f52-5854-4bdd-8d98-c0d06bf0a281",
   "metadata": {},
   "source": [
    "First, you need to choose a garment to simulate.\n",
    "\n",
    "Its template and some auxiliary data should be stored in the .pkl file under `DEFAULTS.data_root/aux_data/garment_dicts/`\n",
    "\n",
    "You can choose from the list of outfits already provided in this folder:\n",
    "\n",
    "![ccraft_garments](static/ccraft_garments.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69656be-c3d7-40e9-8605-fa79fb8ebdf1",
   "metadata": {},
   "source": [
    "Or you can import a new garment from an `.obj` file\n",
    "\n",
    "We also provide `.obj` files for all garments usen in the paper in `DEFAULTS.data_root/aux_data/garment_meshes/` directory.\n",
    "Note that these `.obj` files only have demonstrational purpose. \n",
    "For inference and training we use garment data stored in the .pkl files under `DEFAULTS.data_root/aux_data/garments_dicts/`, not the .obj files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c2e112-63eb-4484-a0b7-8678ac2a3630",
   "metadata": {},
   "source": [
    "## Add your own garment from an `.obj` file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb91aa",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate how to start the simulation from arbitrary SMPL(-X) pose using linear blend-skinning (LBS) initialization. If you don't want to use LBS (for example, you are using non-SMPL(-X) body meshes),  plase see `Inference_from_mesh_sequence.ipynb`.\n",
    "\n",
    "To add a new garment, you should have an `.obj` file with the garment mesh that is **aligned with the canonical body model** (zero-pose and zero-shape) you want to use.\n",
    "\n",
    "We currently support [SMPL](https://smpl.is.tue.mpg.de/) and [SMPL-X](https://smpl-x.is.tue.mpg.de/) body models.\n",
    "\n",
    "Note: you will not be able to run LBS-initialized simulation with a body model that is different from the one you specify in this step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f6ed06-b020-48fe-8e32-034957110def",
   "metadata": {},
   "source": [
    "First, create a .pkl file with your garment data using `add_garment_to_garments_dict` function.\n",
    "\n",
    "It builds a dictionary for the garment that contains:\n",
    "* `rest_pos`: \\[Nx3\\], positions of the vertices in canonical pose that are aligned to zero- pose and shape SMPL body.\n",
    "* `faces`: \\[Fx3\\], triplets of node indices that constitute each face\n",
    "* `node_type` \\[Nx1\\], node type labels (`0` for regular, `3` for \"pinned\"). By default, all nodes are regular, we show how to add \"pinned nodes\" later in this notebook\n",
    "* `lbs` dictionary with shape- and pose- blendshapes and skinning weights for the garment, sampled from SMPL model\n",
    "* `center` and `coarse_edges` info on long-range (coarse) edges used to build hiererchical graph of the garment.\n",
    "\n",
    "To be able to start simulation from arbitrary pose, we use linear blend-skinning (LBS) to initialize the garment geometry in the first frame. For each garment node we sample pose- and shape-blendshapes and skinning weights from the closest SMPL(-X) node in canonical pose.\n",
    "\n",
    "However, for loose garments such approach may result in overly-stretched triangles. Therefore, we use the approach introduced in [\\[Santesteban et al. 2021\\]](http://mslab.es/projects/SelfSupervisedGarmentCollisions/) and average skinning weights and blendshapes over many randomly sampled SMPL(-X) nodes around the given garment node.\n",
    "\n",
    "The parameter `n_samples_lbs` controls the number of random samples to use. We recommend setting it to 0 for tight-fitting garments (shirts, pants) and to 1000 for loose ones (skirts, dresses).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "468c0e51-2d23-46e2-a789-e7757c113292",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling LBS weights...\n",
      "Done.\n",
      "Adding coarse edges... (may take a while)\n",
      "Done.\n",
      "Garment dict saved to /data/agrigorev/02_Projects/ccraft_data/aux_data/garments_dicts/smplx/cindy_020_combined_test.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from utils.mesh_creation import GarmentCreator, add_pinned_verts\n",
    "from utils.defaults import DEFAULTS\n",
    "\n",
    "\n",
    "garment_obj_path = os.path.join(DEFAULTS.aux_data, 'garment_meshes', 'smplx', 'cindy_020_combined.obj')\n",
    "\n",
    "body_models_root = os.path.join(DEFAULTS.aux_data, 'body_models')\n",
    "\n",
    "# Model type, either 'smpl' or 'smplx'\n",
    "model_type = 'smplx'\n",
    "\n",
    "# gender, either 'male`, `female` or `neutral`\n",
    "gender = 'female'\n",
    "\n",
    "garment_dict_path = os.path.join(DEFAULTS.aux_data, 'garments_dicts', model_type, 'cindy_020_combined_test.pkl')\n",
    "\n",
    "\n",
    "# Use approximate_center=True to create temlate faster. In the original paper code it was False\n",
    "gc = GarmentCreator(body_models_root, model_type, gender, \n",
    "                    n_samples_lbs=0, verbose=True, coarse=True, approximate_center=True)\n",
    "gc.add_garment(garment_obj_path, garment_dict_path)\n",
    "\n",
    "# Now garment 'cindy_020_combined_test' is added to the garments_dict.pkl file and can be used in furter steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff85fb86-598b-4b48-baa8-903a89b84636",
   "metadata": {},
   "source": [
    "### Add pinned vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e9df6b-44c8-4c7c-a3d3-3b60d0e1e4bf",
   "metadata": {},
   "source": [
    "For some gaments, it is necessary to fix positions for a subset of nodes relative to the body. For example, fix the top ring of a skirt or pants to prevent it from falling off the body.\n",
    "\n",
    "To label a set of garment nodes as \"pinned\", you need to use `add_pinned_verts` function and provide it with the list of node indices that you want to pin.\n",
    "\n",
    "One easy way of getting indices for a set of nodes, is by using [Blender](https://www.blender.org/). \n",
    "\n",
    "1. Open it, import the garment from the `.obj` file. \n",
    "2. Then in `Layout` tab press `Tab` to go into the editing mode. \n",
    "3. Select all vertices you want to pin. \n",
    "4. Then, go into `Scripting` tab and execute the following piece of code there.\n",
    "\n",
    "```python\n",
    "import bpy\n",
    "import bmesh\n",
    "\n",
    "obj = bpy.context.active_object\n",
    "bm = bmesh.from_edit_mesh(obj.data)    \n",
    "obj = bpy.context.active_object; bm = bmesh.from_edit_mesh(obj.data)    ; selected = [i.index for i in bm.verts if i.select == True]; print(selected)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "5. You will get a list of indices for the selected nodes.\n",
    "\n",
    "Below are pinned indices for the garment `cindy_020_combined`, replace them with yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f4d502-920d-44b2-bcf1-ca0814057c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinned_indices = \\\n",
    "[   26,    27,    28,    36,    47,    56,    68,    69,    79,\n",
    "          93,    94,   103,   121,   130,   151,   161,   185,   197,\n",
    "         222,   237,   262,   279,   280,   307,   326,   353,   372,\n",
    "         402,   422,   455,   456,   478,   513,   538,   576,   603,\n",
    "         641,   670,   706,   738,   778,   812,   855,   891,   936,\n",
    "         973,  1020,  1061,  1109,  1151,  1200,  1243,  1293,  1339,\n",
    "        1390,  1439,  1492,  1543,  1597,  1651,  1706,  1761,  1821,\n",
    "        1878,  1936,  1994,  2052,  2112,  2176,  2238,  2306,  2369,\n",
    "        2439,  2508,  2576,  2644,  2713,  2719,  2784,  2858,  2926,\n",
    "        3001,  3071,  3148,  3218,  3296,  3368,  3448,  3522,  3607,\n",
    "        3687,  3770,  3850,  3934,  4016,  4102,  4187,  4269,  6567,\n",
    "        6626,  7071,  7368,  7577,  7804,  7920,  8040,  8460,  8550,\n",
    "        8632,  8992,  9065,  9137, 11694, 11695, 11700, 11706, 11712,\n",
    "       11715, 11726, 11731, 11735, 11739, 11743, 11747, 11755, 11761,\n",
    "       11770, 11779, 11792, 11797, 11802, 11814, 11819, 11824, 11829,\n",
    "       11851, 11857]\n",
    "\n",
    "add_pinned_verts(garment_dict_path, pinned_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab918a1-0c18-4e0c-a19f-dadd9eb9b44b",
   "metadata": {},
   "source": [
    "# Generate rollout for one sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef67ff-a467-4b3b-820c-83ab53342fb8",
   "metadata": {},
   "source": [
    "Once we have created a garment dict file for our garment (or you can use one of the garments that are already under `DEFAULTS.data_root/aux_data/garment_dicts/`), we can generate a rollout sequence for them using a trained HOOD/ContourCraft model.\n",
    "\n",
    "We provide 3 pretrained models and corresponding configuration files for each of them. The weights of the trained models are located in `DEFAULTS.data_root/trained_models`. The configuration files are in  `DEFAULTS.project_dir/configs`\n",
    "\n",
    "| model file      | config name           | comments                                                                                                                                                                                                                            |\n",
    "|-----------------|-----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| cvpr_submission | hood_cvpr                  | HOOD model used in the CVPR paper. No multi-layer simulation. Use it if you want to compare to the HOOD paper.                                                                                                                                                           |\n",
    "| postcvpr        | hood_final              | Newer HOOD model trained using refactored code with minor bug fixes. No multi-layer simulation. Use it if you want to use HOOD model in a downstream task.|\n",
    "| **contourcraft**        | **contourcraft**              | **Model from the ContourCraft paper. Can simulate multi-layer outfits**. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab84941",
   "metadata": {},
   "source": [
    "## Choose pose sequence "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a890b49",
   "metadata": {},
   "source": [
    "This repository supports inference over .npz sequences from CMU split of AMASS dataset.\n",
    "\n",
    "You can download them [here](https://amass.is.tue.mpg.de/). Use gendered SMPL+H sequences if you want to use SMPL model and gendered SMPL-X sequences if you want to use SMPL-X one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366caa43-f3e1-4ff2-9f4f-02087a09fc80",
   "metadata": {},
   "source": [
    "### create validation config and create `Runner` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3eb786-1682-4894-995a-366dd7c92e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/agrigorev/Workdir/contourcraft_private/utils/validation.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd = torch.load(checkpoint_path)\n"
     ]
    }
   ],
   "source": [
    "from utils.validation import apply_material_params\n",
    "from utils.validation import load_runner_from_checkpoint\n",
    "from utils.arguments import load_params\n",
    "from utils.common import move2device\n",
    "from utils.io import pickle_dump\n",
    "from utils.defaults import DEFAULTS\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Set material paramenters, see configs/cvpr.yaml for the training ranges for each parameter\n",
    "material_dict = dict()\n",
    "material_dict['density'] = 0.20022\n",
    "material_dict['lame_mu'] = 23600.0\n",
    "material_dict['lame_lambda'] = 44400\n",
    "material_dict['bending_coeff'] = 3.962e-05\n",
    "\n",
    "\n",
    "# ====================================================================================================\n",
    "\n",
    "models_dir = Path(DEFAULTS.data_root) / 'trained_models'\n",
    "\n",
    "# Choose the model and the configuration file\n",
    "\n",
    "# config_name = 'hood_cvpr'\n",
    "# checkpoint_path = models_dir / 'hood_cvpr.pth'\n",
    "\n",
    "# config_name = 'hood_final'\n",
    "# checkpoint_path = models_dir / 'hood_final.pth'\n",
    "\n",
    "config_name = 'contourcraft'\n",
    "checkpoint_path = models_dir / 'contourcraft.pth'\n",
    "\n",
    "\n",
    "# ====================================================================================================\n",
    "\n",
    "\n",
    "# load the config from .yaml file and load .py modules specified there\n",
    "modules, experiment_config = load_params(config_name)\n",
    "\n",
    "# modify the config to use it in validation \n",
    "experiment_config = apply_material_params(experiment_config, material_dict)\n",
    "\n",
    "# load Runner object and the .py module it is declared in\n",
    "runner_module, runner = load_runner_from_checkpoint(checkpoint_path, modules, experiment_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d6421-9e94-41a9-83fb-6373ef431e18",
   "metadata": {},
   "source": [
    "### create one-sequence dataloader\n",
    "\n",
    "Here you'll need to choose a garment by setting `garment_name` variable. The garment name should correspond to a `.pkl` file under `DEFAULTS.data_root/aux_data/garments_dicts/`\n",
    "\n",
    "Note that it can also be a comma-separated list of garments, then they'll be combined into a single outfit. For example:\n",
    "```\n",
    "garment_name = 'smplx/cindy_020::bottom_skirt, smplx/cindy_020::top_blouse'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd0e9c8",
   "metadata": {},
   "source": [
    "**To test for SMPL poses, use**\n",
    "\n",
    "```\n",
    "sequence_path =  'path/to/SMPLH/AMASS/CMU/01/01_01_poses.npz'\n",
    "garment_name = 'longskirt'\n",
    "sequence_loader = 'cmu_npz_smpl'\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2e00cfc-0602-4109-9817-5d0690ab0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file with the pose sequence\n",
    "from utils.validation import create_postcvpr_one_sequence_dataloader\n",
    "\n",
    "# If True, the SMPL(-X) poses are slightly modified to avoid hand-body self-penetrations. The technique is adopted from the code of SNUG \n",
    "separate_arms = True\n",
    "\n",
    "# path to the pose sequence file\n",
    "# CMU_path = 'path/to/AMASS/CMU'\n",
    "CMU_path = '/data/agrigorev/00_Datasets/AMASS/smplx/CMU'\n",
    "sequence_path =  Path(CMU_path) / '01/01_01_stageii.npz'\n",
    "\n",
    "# name of the garment to simulate \n",
    "# should correspond to a pkl file under DEFAULTS.data_root/aux_data/garments_dicts/\n",
    "garment_name = 'smplx/cindy_020_combined_test'\n",
    "\n",
    "# It can be a comma-separated list of individual garments\n",
    "# garment_name = 'smplx/cindy_020::bottom_skirt, smplx/cindy_020::top_blouse'\n",
    "\n",
    "\n",
    "# gender of the body model, sould be the same as the one used to create the garment\n",
    "gender = 'female'\n",
    "\n",
    "# ====================================================================================================\n",
    "\n",
    "# Choose the type of the pose sequence you want to use: 'cmu_npz_smpl', 'cmu_npz_smplx'\n",
    "\n",
    "# to use AMASS SMPL-X pose sequence\n",
    "sequence_loader = 'cmu_npz_smplx'\n",
    "\n",
    "# to use AMASS SMPL pose sequence\n",
    "# sequence_loader = 'cmu_npz_smpl\n",
    "\n",
    "\n",
    "\n",
    "# ====================================================================================================\n",
    "\n",
    "\n",
    "dataloader = create_postcvpr_one_sequence_dataloader(sequence_path, garment_name, sequence_loader=sequence_loader, \n",
    "                                            obstacle_dict_file=None, gender=gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29d952c3-0def-410e-b152-e9e3dd99e11a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:38<00:00,  5.22it/s]\n"
     ]
    }
   ],
   "source": [
    "sequence = next(iter(dataloader))\n",
    "sequence = move2device(sequence, 'cuda:0')\n",
    "trajectories_dict = runner.valid_rollout(sequence,  bare=True, n_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3476f3-3b26-4224-b27a-4cbfc4ed0001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rollout saved into /data/agrigorev/02_Projects/ccraft_data/temp/output_hood_old.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the sequence to disk\n",
    "out_path = Path(DEFAULTS.data_root) / 'temp' / 'output.pkl'\n",
    "print(f\"Rollout saved into {out_path}\")\n",
    "pickle_dump(dict(trajectories_dict), out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733d150e-aa88-420a-b22c-02c561f205aa",
   "metadata": {},
   "source": [
    "### write a video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64485f0f-55ef-45e2-b64a-9ef2e5fc27c6",
   "metadata": {},
   "source": [
    "Finally, we can render a video of the generated sequence with [aitviewer](https://github.com/eth-ait/aitviewer)\n",
    "\n",
    "Or you can render it interactively using `python utils/show.py rollout_path=PATH_TO_SEQUENCE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e8d616f-f90f-4c19-bc9a-6980da5718e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/agrigorev/miniforge3/envs/ccraft/lib/python3.10/site-packages/trimesh/geometry.py:7: UserWarning: A NumPy version >=1.23.5 and <2.5.0 is required for this version of SciPy (detected version 1.23.1)\n",
      "  import scipy.sparse\n"
     ]
    }
   ],
   "source": [
    "from utils.show import write_video \n",
    "from aitviewer.headless import HeadlessRenderer\n",
    "\n",
    "# Careful!: creating more that one renderer in a single session causes an error\n",
    "renderer = HeadlessRenderer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c43b212-2ec8-4459-9442-a1dae7eb349c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out_path \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(DEFAULTS\u001b[38;5;241m.\u001b[39mdata_root) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m out_video \u001b[38;5;241m=\u001b[39m Path(DEFAULTS\u001b[38;5;241m.\u001b[39mdata_root) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m write_video(out_path, out_video, renderer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "out_path = Path(DEFAULTS.data_root) / 'temp' / 'output.pkl'\n",
    "out_video = Path(DEFAULTS.data_root) / 'temp' / 'output.mp4'\n",
    "write_video(out_path, out_video, renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9658210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccraft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
